{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Imports & Settings\n",
        "All imports are consolidated here. We also configure pandas display options to aid debugging (optional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: more helpful DataFrame display for interactive work\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "pd.set_option('display.width', 120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# File path (note the use of raw string `r` to handle backslashes)\n",
        "file_path = r\"../../data/speeches.csv\"  \n",
        "\n",
        "# Define the base path for outputs\n",
        "base_path = r\"../../data/\"  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n",
        "Reads the CSV into a pandas DataFrame `df`. The variable name is kept unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Quick sanity check (uncomment to preview)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Party Mapping & Initial Features\n",
        "Creates a `Party` column based on `factionId` and computes a first pass of `count_words`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping factionId to party names\n",
        "party_map = {\n",
        "    0: \"AFD\",\n",
        "    3: \"Greens\",\n",
        "    4: \"CDU/CSU\",\n",
        "    6: \"DIE LINKE.\",\n",
        "    13: \"FDP\",\n",
        "    23: \"SPD\"\n",
        "}\n",
        "\n",
        "# Create a new 'Party' column based on 'factionId'\n",
        "df['Party'] = df['factionId'].map(party_map)\n",
        "\n",
        "# Count words in 'speechContent' column (robust to NaNs)\n",
        "df['count_words'] = df['speechContent'].fillna('').astype(str).str.count(r'\\S+')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Analysis Dataset `df_test`\n",
        "Filters out short speeches and standardizes certain `positionShort` values for specific speakers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## create dataset used for the analysis\n",
        "df_test = df[df['count_words'] > 250].copy()\n",
        "\n",
        "# Update Eva Högl entries\n",
        "mask_eva = (\n",
        "    (df_test['firstName'] == 'Eva') &\n",
        "    (df_test['lastName'] == 'Högl') &\n",
        "    (df_test['positionShort'] == 'Not found')\n",
        ")\n",
        "df_test.loc[mask_eva, 'positionShort'] = 'Member of Parliament'\n",
        "\n",
        "# Update Anja Karliczek entries\n",
        "mask_anja = (\n",
        "    (df_test['firstName'] == 'Anja') &\n",
        "    (df_test['lastName'] == 'Karliczek') &\n",
        "    (df_test['positionShort'] == 'Not found')\n",
        ")\n",
        "df_test.loc[mask_anja, 'positionShort'] = 'Minister'\n",
        "\n",
        "# Update Marco Wanderwitz entries\n",
        "mask_marco = (\n",
        "    (df_test['firstName'] == 'Marco') &\n",
        "    (df_test['lastName'] == 'Wanderwitz') &\n",
        "    (df_test['positionShort'] == 'Not found')\n",
        ")\n",
        "df_test.loc[mask_marco, 'positionShort'] = 'Secretary of State'\n",
        "\n",
        "# Remove entries where positionShort is undesired\n",
        "df_test = df_test[~df_test['positionShort'].isin(['Presidium of Parliament', 'Not found', 'Guest'])].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Text Cleaning – Numbers in Brackets & Whitespace\n",
        "Removes `({0})`, `({1})`, etc., collapses whitespace, and trims."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## delete bracketed numbers like ({0}), ({1}), etc. and replace multiple whitespace/newlines with single spaces\n",
        "def clean_speech_content(text: str) -> str:\n",
        "    # Guard for non-str inputs\n",
        "    if text is None:\n",
        "        return ''\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove bracketed numbers like ({0}), ({1}), etc.\n",
        "    text = re.sub(r'\\(\\{[0-9]+\\}\\)', '', text)\n",
        "\n",
        "    # Replace multiple whitespace/newlines with single spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the speechContent column\n",
        "df_test['speechContent'] = df_test['speechContent'].apply(clean_speech_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Remove Preambles/Closings from `speechContent`\n",
        "Deletes salutations, formal addresses, and common closing thanks using a curated phrase list. Longer forms are matched before shorter ones to avoid partial-match leftovers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## clean up speechContent column by deleting salutations and other non-speech content\n",
        "def remove_phrases(text: str) -> str:\n",
        "    if text is None:\n",
        "        return ''\n",
        "    text = str(text)\n",
        "\n",
        "    # List of phrases to remove - organized by category and sorted logically\n",
        "    phrases_to_remove = [\n",
        "        # Presidential addresses\n",
        "        \"Sehr geehrter Herr Präsident\",\n",
        "        \"Sehr geehrte Frau Präsidentin\",\n",
        "        \"Herr Präsident\",\n",
        "        \"Frau Präsidentin\",\n",
        "        \"Frau Präsident\",\n",
        "\n",
        "        # General formal addresses (longest first - CRITICAL ORDER)\n",
        "        \"Meine sehr verehrten Damen und Herren\",\n",
        "        \"Meine sehr geehrten Damen und Herren\",\n",
        "        \"Sehr geehrte Damen und Herren\",\n",
        "        \"Sehr verehrte Damen und Herren\",\n",
        "        \"Meine Damen und Herren\",\n",
        "        \"Meine Herren und Damen\",\n",
        "        \"Meine Damen! Meine Herren\",\n",
        "        \"Meine sehr verehrten Damen\",\n",
        "        \"Meine Damen\",\n",
        "        \"Meine Herren\",\n",
        "\n",
        "        # Colleague addresses (longest first - CRITICAL ORDER)\n",
        "        \"Meine sehr geehrten Kolleginnen und Kollegen\",\n",
        "        \"Meine sehr verehrten Kolleginnen und Kollegen\",\n",
        "        \"Meine lieben Kolleginnen und Kollegen\",\n",
        "        \"Meine verehrten Kolleginnen und Kollegen\",\n",
        "        \"Liebe Kolleginnen und Kollegen\",\n",
        "        \"Verehrte Kolleginnen und Kollegen\",\n",
        "        \"Geehrte Kolleginnen und Kollegen\",\n",
        "        \"Sehr geehrte Kolleginnen und Kollegen\",\n",
        "        \"Sehr verehrte Kolleginnen und Kollegen\",\n",
        "        \"Liebe Kolleginnen, liebe Kollegen\",\n",
        "        \"Sehr geehrte Kollegen\",\n",
        "\n",
        "        # Individual colleague addresses (with one word after)\n",
        "        r\"Lieber Kollege \\w+\",\n",
        "        r\"Liebe Kollegin \\w+\",\n",
        "        r\"Herr Kollege \\w+\",\n",
        "        r\"Frau Kollegin \\w+\",\n",
        "\n",
        "        # These must come AFTER longer phrases that contain them\n",
        "        \"Sehr geehrte\",\n",
        "        \"Sehr geehrter\",\n",
        "        \"Sehr verehrte\",\n",
        "        \"Kollegen\",\n",
        "\n",
        "        # Other addresses\n",
        "        \"Sehr verehrte Gäste\",\n",
        "\n",
        "        # Thanks and closing phrases\n",
        "        \"Ich danke für Ihre Aufmerksamkeit\",\n",
        "        \"Ich danke für die Aufmerksamkeit\",\n",
        "        \"Herzlichen Dank\",\n",
        "        \"Vielen Dank\",\n",
        "        \"Danke\",\n",
        "    ]\n",
        "\n",
        "    # Remove each phrase with flexible punctuation\n",
        "    for phrase in phrases_to_remove:\n",
        "        # Check if phrase contains regex pattern (for phrases with \\w+)\n",
        "        if r\"\\w+\" in phrase:\n",
        "            # For patterns like \"Lieber Kollege \\w+\"\n",
        "            pattern = phrase + r\"\\s*[.,!?;:]*\"\n",
        "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "        else:\n",
        "            # For regular phrases - use word boundaries to prevent partial matches\n",
        "            pattern = r\"\\b\" + re.escape(phrase) + r\"\\b\\s*[.,!?;:]*\"\n",
        "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Clean up any remaining single letters at the beginning of sentences\n",
        "    text = re.sub(r\"\\b[a-zA-Z]\\s+\", ' ', text)\n",
        "\n",
        "    # Clean up any remaining \"sehr\" at the beginning of sentences or after punctuation\n",
        "    text = re.sub(r\"(^|\\s)sehr\\s+\", r\"\\1\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Clean up extra whitespace\n",
        "    text = re.sub(r\"\\s+\", ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply the function\n",
        "df_test['speechContent'] = df_test['speechContent'].apply(remove_phrases)\n",
        "print(\"Updated phrase removal completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Recompute Word Counts\n",
        "Rebuilds `count_words` on the cleaned `speechContent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add variable to count words in 'speechContent' column\n",
        "df_test['count_words'] = df_test['speechContent'].fillna('').astype(str).str.count(r'\\S+')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Split by Electoral Term\n",
        "Creates a separate dataset for each unique `electoralTerm` and exposes them in the notebook's global scope\n",
        "following your original naming convention (e.g., `electoralTerm_19`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all unique electoral terms\n",
        "electoral_terms = sorted(df_test['electoralTerm'].unique())\n",
        "\n",
        "# Create individual datasets with globals()\n",
        "for term in electoral_terms:\n",
        "    dataset_name = f'electoralTerm_{term}'\n",
        "    globals()[dataset_name] = df_test[df_test['electoralTerm'] == term]\n",
        "    print(f\"{dataset_name}: {len(globals()[dataset_name])} entries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Outputs\n",
        "Writes the cleaned master dataset and each electoral-term-specific dataset to CSV.\n",
        "Paths are parameterized via `base_path`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save main dataset\n",
        "df_test.to_csv(f\"{base_path}\\\\df_test_cleaned.csv\", index=False)\n",
        "\n",
        "## Save electoral term datasets\n",
        "for term in sorted(df_test['electoralTerm'].unique()):\n",
        "    dataset_name = f'electoralTerm_{term}'\n",
        "    if dataset_name in globals():\n",
        "        globals()[dataset_name].to_csv(f\"{base_path}\\\\{dataset_name}.csv\", index=False)\n",
        "        print(f\"Saved {dataset_name}.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
