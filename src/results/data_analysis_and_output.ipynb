{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "178189c5",
      "metadata": {},
      "source": [
        "# Data Analysis & Output – Notebook\n",
        "\n",
        "Combines GPT and Gemini scores, computes derived metrics, explores distributions/correlations, and produces figures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fd68f3",
      "metadata": {},
      "source": [
        "## 0) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a12dc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr, spearmanr, kruskal\n",
        "import itertools\n",
        "\n",
        "# Plot settings (optional)\n",
        "sns.set(rc={\"figure.dpi\": 120})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cefcb4ae",
      "metadata": {},
      "source": [
        "## 1) Load & Merge Data\n",
        "Reads analysis, base, and Gemini score files. Merges into `df_combined` and adds derived columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6116171e",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Define the file path (make sure to use raw string or double backslashes)\n",
        "file_path = r\"../../data/electoralTerm_19_analysis.csv\"\n",
        "\n",
        "## Load the CSV into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "## Load the base dataframe\n",
        "base_path = r\"../../data/electoralTerm_19_scored.csv\"\n",
        "df_base = pd.read_csv(base_path)\n",
        "\n",
        "## Load the Gemini 2.0 scores\n",
        "gemini_2_path = r\"../../data/electoralTerm_19_scored_gemini.csv\"\n",
        "df_gemini_2 = pd.read_csv(gemini_2_path)[['id', 'gemini_score']].rename(columns={'gemini_score': 'gemini_2.0_score'})\n",
        "\n",
        "## Load the Gemini 2.5 scores\n",
        "gemini_2_5_path = r\"../../data/electoralTerm_19_scored_gemini_2_5.csv\"\n",
        "df_gemini_2_5 = pd.read_csv(gemini_2_5_path)[['id', 'gemini_score']].rename(columns={'gemini_score': 'gemini_2.5_score'})\n",
        "\n",
        "## Merge all dataframes on 'id'\n",
        "df_combined = df_base.merge(df_gemini_2, on='id', how='left')\n",
        "df_combined = df_combined.merge(df_gemini_2_5, on='id', how='left')\n",
        "\n",
        "## Create calculated columns\n",
        "df_combined['gpt_average'] = df_combined[['gpt_score', 'gpt_4.1_score']].mean(axis=1)  # average of gpt_score and gpt_4.1_score\n",
        "df_combined['gpt_deviation'] = (df_combined['gpt_score'] - df_combined['gpt_4.1_score']).abs()  # deviation between gpt_score and gpt_4.1_score\n",
        "\n",
        "df_combined['gemini_average'] = df_combined[['gemini_2.0_score', 'gemini_2.5_score']].mean(axis=1)  # average of gemini_2.0_score and gemini_2.5_score\n",
        "df_combined['gemini_deviation'] = (df_combined['gemini_2.0_score'] - df_combined['gemini_2.5_score']).abs()  # deviation between gemini_2.0_score and gemini_2.5_score\n",
        "df_combined['gpt_gemini_deviation'] = df_combined['gpt_average'] - df_combined['gemini_average']  # deviation between gpt_average and gemini_average\n",
        "\n",
        "# Peek at structure\n",
        "display(df_combined.shape)\n",
        "display(df_combined[['gpt_score', 'gpt_4.1_score', 'gemini_2.0_score', 'gemini_2.5_score']].describe())\n",
        "display(df_combined['Party'].value_counts())\n",
        "print(df_combined.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c5845f",
      "metadata": {},
      "source": [
        "## 2) Descriptive Statistics per Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dea805",
      "metadata": {},
      "outputs": [],
      "source": [
        "## descriptive stats per model\n",
        "# List of model score columns\n",
        "model_cols = ['gpt_score', 'gpt_4.1_score', 'gemini_2.0_score', 'gemini_2.5_score']\n",
        "\n",
        "# Basic descriptive statistics\n",
        "descriptive_stats = df_combined[model_cols].describe().T  # Transpose for readability\n",
        "display(descriptive_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5e96f5",
      "metadata": {},
      "source": [
        "## 3) Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b81419",
      "metadata": {},
      "outputs": [],
      "source": [
        "## distribution plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "for col in model_cols:\n",
        "    sns.kdeplot(df_combined[col], label=col, fill=True, common_norm=False)\n",
        "plt.title('Ideological Score Distributions by Model')\n",
        "plt.xlabel('Ideological Score (1–10)')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ba0a21",
      "metadata": {},
      "source": [
        "## 4) Inter-Model Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9500a783",
      "metadata": {},
      "outputs": [],
      "source": [
        "## inter-model correlation\n",
        "# Pearson correlation\n",
        "correlation_matrix = df_combined[model_cols].corr(method='pearson')\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Pairwise Pearson r, p, df\n",
        "results = []\n",
        "for col1, col2 in itertools.combinations(model_cols, 2):\n",
        "    data1 = df_combined[col1].dropna()\n",
        "    data2 = df_combined[col2].dropna()\n",
        "    # Align by index after dropping NaNs\n",
        "    df_temp = pd.concat([data1, data2], axis=1).dropna()\n",
        "    r, p = pearsonr(df_temp[col1], df_temp[col2])\n",
        "    df_value = len(df_temp) - 2  # degrees of freedom\n",
        "    results.append((col1, col2, r, p, df_value))\n",
        "corr_df = pd.DataFrame(results, columns=['Variable 1', 'Variable 2', 'r', 'p-value', 'df'])\n",
        "print(corr_df)\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=0.8, vmax=1)\n",
        "plt.title('Pearson Correlation Matrix: Model Ideological Scores')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f1ebf6b",
      "metadata": {},
      "source": [
        "## 5) Model Agreement & Variability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57aeb2ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "## model agreement & variability\n",
        "# Add a column for inter-model deviation\n",
        "df_combined['model_std'] = df_combined[model_cols].std(axis=1)\n",
        "\n",
        "# View basic stats\n",
        "print(df_combined['model_std'].describe())\n",
        "\n",
        "# Plot\n",
        "sns.histplot(df_combined['model_std'], bins=30, kde=True)\n",
        "plt.title('Distribution of Inter-Model Standard Deviation per Speech')\n",
        "plt.xlabel('Standard Deviation Across Models')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d749e006",
      "metadata": {},
      "source": [
        "## 6) Bland–Altman: GPT vs Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1dcf871",
      "metadata": {},
      "outputs": [],
      "source": [
        "## bland-altman plot of GPT vs Gemini\n",
        "model_a = df_combined['gpt_average']\n",
        "model_b = df_combined['gemini_average']\n",
        "\n",
        "# Calculate means and differences\n",
        "mean_scores = (model_a + model_b) / 2\n",
        "diff_scores = model_a - model_b\n",
        "mean_diff = np.mean(diff_scores)\n",
        "std_diff = np.std(diff_scores)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(mean_scores, diff_scores, alpha=0.5)\n",
        "plt.axhline(mean_diff, linestyle='--', label=f'Mean diff = {mean_diff:.2f}')\n",
        "plt.axhline(mean_diff + 1.96*std_diff, linestyle='--', label='+1.96 SD')\n",
        "plt.axhline(mean_diff - 1.96*std_diff, linestyle='--', label='-1.96 SD')\n",
        "plt.title('Bland-Altman Plot: GPT vs Gemini')\n",
        "plt.xlabel('Mean Score (GPT & Gemini)')\n",
        "plt.ylabel('Difference (GPT - Gemini)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cad67e4",
      "metadata": {},
      "source": [
        "## 7) Speech Length vs Inter-Model Deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bad477b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## length vs deviation\n",
        "# Basic scatterplot with trendline\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df_combined, x='count_words', y='model_std', alpha=0.3)\n",
        "sns.regplot(data=df_combined, x='count_words', y='model_std', scatter=False)\n",
        "plt.title('Relationship Between Speech Length and Inter-Model Deviation')\n",
        "plt.xlabel('Word Count (count_words)')\n",
        "plt.ylabel('Standard Deviation Across Models (model_std)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "pearson_corr, pearson_p = pearsonr(df_combined['count_words'], df_combined['model_std'])\n",
        "spearman_corr, spearman_p = spearmanr(df_combined['count_words'], df_combined['model_std'])\n",
        "print(f\"Pearson correlation: r = {pearson_corr:.3f}, p = {pearson_p:.3e}\")\n",
        "print(f\"Spearman correlation: r = {spearman_corr:.3f}, p = {spearman_p:.3e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a89f3e2b",
      "metadata": {},
      "source": [
        "## 8) Party-Level Analysis & Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b4f38cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "## party level analysis\n",
        "score_cols = ['gpt_score', 'gpt_4.1_score', 'gemini_2.0_score', 'gemini_2.5_score']\n",
        "\n",
        "party_stats = df_combined.groupby('Party')[score_cols].agg({\n",
        "    'gpt_score': ['mean', 'std', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)],\n",
        "    'gpt_4.1_score': ['mean', 'std', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)],\n",
        "    'gemini_2.0_score': ['mean', 'std', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)],\n",
        "    'gemini_2.5_score': ['mean', 'std', 'median', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)]\n",
        "})\n",
        "print(party_stats)\n",
        "\n",
        "# Boxplot by party (note: plotting gemini_2.5_score per your script)\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(data=df_combined, x='Party', y='gemini_2.5_score')\n",
        "plt.title('Distribution of GPT-4.1 Ideological Scores by Party')\n",
        "plt.ylabel('Ideological Score (1 = far left, 10 = far right)')\n",
        "plt.xlabel('Political Party')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Group by party to inspect differences\n",
        "model_diff = df_combined.groupby('Party')[['gpt_average', 'gemini_average', 'gpt_gemini_deviation']].mean().reset_index()\n",
        "print(model_diff)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for model in ['gpt_average', 'gemini_average']:\n",
        "    plt.plot(model_diff['Party'], model_diff[model], marker='o', label=model)\n",
        "plt.axhline(5, linestyle='--', alpha=0.7)\n",
        "plt.ylabel('Average Score')\n",
        "plt.xlabel('Party')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Radar plot\n",
        "labels = model_diff['Party'].tolist()\n",
        "models = ['gpt_average', 'gemini_average']\n",
        "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
        "angles += angles[:1]\n",
        "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "for model in models:\n",
        "    values = model_diff[model].tolist()\n",
        "    values += values[:1]\n",
        "    ax.plot(angles, values, label=model)\n",
        "    ax.fill(angles, values, alpha=0.1)\n",
        "ax.set_theta_offset(np.pi / 2)\n",
        "ax.set_theta_direction(-1)\n",
        "ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "plt.title('Radar Plot of Average Ideological Scores by Party')\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed6bd4e2",
      "metadata": {},
      "source": [
        "## 9) Normality Checks & Non-Parametric Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6f2700",
      "metadata": {},
      "outputs": [],
      "source": [
        "## check for normality assumption\n",
        "# Histogram with KDE: GPT-4.1 Scores\n",
        "sns.histplot(df_combined['gpt_4.1_score'], kde=True)\n",
        "plt.title(\"Histogram with KDE: GPT-4.1 Scores\")\n",
        "plt.xlabel(\"Ideological Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Q-Q plot for GPT-4.1 scores\n",
        "import scipy.stats as stats\n",
        "stats.probplot(df_combined['gpt_4.1_score'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q Plot: GPT-4.1 Scores\")\n",
        "plt.show()\n",
        "\n",
        "# Kruskal–Wallis across parties for GPT-4.1\n",
        "groups = [df_combined[df_combined['Party'] == party]['gpt_4.1_score'] for party in df_combined['Party'].unique()]\n",
        "stat, p = kruskal(*groups)\n",
        "print(f\"Kruskal-Wallis H = {stat:.3f}, p = {p:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
