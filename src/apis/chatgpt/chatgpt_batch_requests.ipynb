{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0cd4c284",
      "metadata": {},
      "source": [
        "## 0. Imports & Setup\n",
        "Consolidate imports and set up the OpenAI client. API keys are read from environment variables to avoid accidental commits.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f803c3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Configure API key from environment (preferred for security)\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Sanity check (will be None if not set yet)\n",
        "print(\"OPENAI_API_KEY set:\", bool(openai.api_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb8eb0d",
      "metadata": {},
      "source": [
        "## 1. Configuration (placeholders)\n",
        "Update the placeholders below before running. Names are preserved exactly as in your script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371a580a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CONFIGURATION ===\n",
        "INPUT_CSV = r\"../../../data/electoralTerm_19.csv\" #replace with different file path if needed\n",
        "CHUNK_SIZE = 500  # ‚¨ÖÔ∏è Now splitting into 500-row chunks\n",
        "CHUNKS_DIR = \"batch_chunks_500\"\n",
        "MODEL = \"gpt-4o-mini\" #adjust model if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c57704",
      "metadata": {},
      "source": [
        "## 2. Ensure Output Directory & Load CSV\n",
        "Creates the chunks directory (if needed) and loads the input data to `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7da581",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Ensure output directory exists ===\n",
        "os.makedirs(CHUNKS_DIR, exist_ok=True)\n",
        "\n",
        "# === Load CSV ===\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "print(f\"üìÑ Loaded {len(df)} speeches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa2bcb4",
      "metadata": {},
      "source": [
        "## 3. Prompt Template\n",
        "Defines `build_prompt(text)` to produce a consistent classification instruction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e98bbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Prompt template ===\n",
        "def build_prompt(text):\n",
        "    return f\"\"\"You are a political analyst trained to classify political texts on a 1-10 left-right ideological spectrum. Your task is to read a political speech or excerpt and assign a number from 1 (far-left) to 10 (far-right) based on its ideological content.\n",
        "\n",
        "\n",
        "Use this scale as a reference, based on common positions in German politics:\n",
        "\n",
        "\n",
        "1: Far-left, revolutionary socialism (e.g., MLPD, Antifa)  \n",
        "2: Anti-capitalist democratic socialism (e.g., DIE LINKE, radical wing)  \n",
        "3: Progressive left, social justice-focused (e.g., DIE LINKE, moderate)  \n",
        "4: Center-left, reformist social democracy (e.g., SPD)  \n",
        "5: Centrist, socially liberal or pragmatic (e.g., Volt, FDP, left-liberal)  \n",
        "6: Center-right liberalism, market-oriented (e.g., FDP, neoliberal wing)  \n",
        "7: Conservative mainstream (e.g., CDU/CSU)  \n",
        "8: National-conservative or traditionalist right (e.g., WerteUnion)  \n",
        "9: Right-wing populist, anti-immigration (e.g., AfD, moderate)  \n",
        "10: Far-right nationalist or extremist (e.g., AfD, radical wing)\n",
        "\n",
        "\n",
        "Classify the following political text according to this scale and output **only the numeric value (1-10)**. Do not add explanations, comments, or any additional text.\n",
        "\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5621ff84",
      "metadata": {},
      "source": [
        "## 4. Create JSONL Chunks\n",
        "Splits the dataset into `CHUNK_SIZE` slices and writes one JSONL file per slice inside `CHUNKS_DIR`.\n",
        "Each line contains a request object compatible with the Batch API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e4e29d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Chunk and write JSONL files ===\n",
        "chunks = [df.iloc[i:i + CHUNK_SIZE] for i in range(0, len(df), CHUNK_SIZE)]\n",
        "print(f\"üß© Splitting into {len(chunks)} chunks of {CHUNK_SIZE} rows each\")\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    jsonl_path = os.path.join(CHUNKS_DIR, f\"chunk_{i:03d}.jsonl\")\n",
        "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for idx, row in chunk.iterrows():\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a political classification assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": build_prompt(row['speechContent'])}\n",
        "            ]\n",
        "            task = {\n",
        "                \"custom_id\": str(row.name),  # Keep global index\n",
        "                \"method\": \"POST\",\n",
        "                \"url\": \"/v1/chat/completions\",\n",
        "                \"body\": {\n",
        "                    \"model\": MODEL,\n",
        "                    \"temperature\": 0.05,\n",
        "                    \"messages\": messages\n",
        "                }\n",
        "            }\n",
        "            f.write(json.dumps(task) + \"\\n\")\n",
        "    print(f\"‚úÖ Saved chunk {i+1} ‚Üí {jsonl_path} with {len(chunk)} tasks\")\n",
        "\n",
        "print(\"üéâ Done! You can now submit each JSONL to OpenAI Batch API.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce0a179d",
      "metadata": {},
      "source": [
        "## 5. Submit a Batch Job\n",
        "Uploads a selected JSONL chunk and creates a batch (24h window by default). Note: Batch jobs were chosen because of cheaper tokens. On a paid plan, also possible to run analysis directly without batch limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034d89f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Submit batch job to OpenAI ===\n",
        "import openai  \n",
        "\n",
        "# Choose which chunk to upload (edit as needed)\n",
        "jsonl_to_upload = os.path.join(CHUNKS_DIR, \"chunk_000.jsonl\")\n",
        "\n",
        "# Upload the file for batch processing\n",
        "upload = openai.files.create(\n",
        "    file=open(jsonl_to_upload, \"rb\"),\n",
        "    purpose=\"batch\"\n",
        ")\n",
        "file_id = upload.id\n",
        "print(f\"‚úÖ File uploaded: {file_id}\")\n",
        "\n",
        "# Create the batch job\n",
        "batch = openai.batches.create(\n",
        "    input_file_id=file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\" \n",
        ")\n",
        "\n",
        "batch_id = batch.id\n",
        "print(f\"üöÄ Batch job submitted! Batch ID: {batch_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67577b39",
      "metadata": {},
      "source": [
        "## 6. Retrieve Result File ID from an Existing Batch\n",
        "If you already have a `batch_id`, you can retrieve its status and the `output_file_id` (when completed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca0f691",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Retrieve file id from batch job ===\n",
        "batch_id = \"<REPLACE_WITH_YOUR_BATCH_ID>\"  # ‚Üê insert batch job ID (found in output of previous code chunk)\n",
        "\n",
        "batch_info = openai.batches.retrieve(batch_id)\n",
        "\n",
        "if batch_info.status == \"completed\":\n",
        "    result_file_id = batch_info.output_file_id\n",
        "    print(\"üéâ Here's the file ID:\", result_file_id)\n",
        "else:\n",
        "    print(\"Batch not completed yet. Current status:\", batch_info.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d35cb02",
      "metadata": {},
      "source": [
        "## 7. Merge Scores Back Into CSV\n",
        "Downloads the batch results and updates the CSV's `gpt_score` column. Only missing values are updated.\n",
        "\n",
        "**Note**: This cell preserves your variable names and fixes a small function indentation bug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c356c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1. Config ===\n",
        "csv_path = r\"../../../data/electoralTerm_19_scored.csv\" #replace with different file path if needed\n",
        "result_file_id = result_file_id  # assumes it's set in the previous cell when batch completed\n",
        "\n",
        "# === 2. Load the CSV ===\n",
        "df = pd.read_csv(csv_path)\n",
        "df['custom_id'] = df['custom_id'].astype(str)  # Ensure matching format\n",
        "\n",
        "# === 3. Retrieve the result file content from OpenAI ===\n",
        "result_text = openai.files.retrieve_content(result_file_id)\n",
        "\n",
        "# === 4. Parse JSONL into a lookup dict: {custom_id: content}\n",
        "custom_id_to_score = {}\n",
        "for line in result_text.splitlines():\n",
        "    try:\n",
        "        entry = json.loads(line)\n",
        "        custom_id = str(entry.get('custom_id'))\n",
        "        content = entry['response']['body']['choices'][0]['message']['content']\n",
        "        custom_id_to_score[custom_id] = content\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Skipping malformed line: {e}\")\n",
        "\n",
        "# === 5. Update only missing gpt_score values ===\n",
        "def update_score(row):\n",
        "    return custom_id_to_score.get(str(row['custom_id']), row.get('gpt_score'))\n",
        "\n",
        "df['gpt_score'] = df.apply(update_score, axis=1)\n",
        "\n",
        "# === 6. Save the updated CSV ===\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"‚úÖ gpt_score column updated using result_file_id.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
